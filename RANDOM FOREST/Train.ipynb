{"cells":[{"cell_type":"markdown","metadata":{"id":"CO1h7vXH3GqR"},"source":["## Preparation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rz4pAsIx3Gq4"},"outputs":[],"source":["import r2pipe as radare\n","import pandas as pd\n","from ember.features import PEFeatureExtractor\n","import random\n","import glob\n","import os\n","from tqdm import tqdm\n","import sys\n","from joblib import dump, load\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.utils import Bunch\n","import csv\n","import numpy as np\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","def get_ember_features(path, dst=None):\n","  \"\"\"\n","  This function extract features from a binary file ad puts them in a pandas dataframe\n","  Extracted features:\n","    -ByteHistogram\n","    -ByteEntropyHistogram\n","    -StringsInfo\n","    -GeneralFileInfo\n","    -HeaderFileInfo\n","    -SectionInfo\n","    -ImportsInfo\n","    -ExportsInfo\n","\n","  @param path the path where the binary is located\n","  @return dataframe the pandas dataframe where the extracted features are located\n","  \"\"\"\n","  with open(path, 'rb') as f:\n","    extractor = PEFeatureExtractor()\n","    bytes = f.read()\n","    features = [float(value) for value in extractor.feature_vector(bytes)]\n","    features_df = {'feature_' + str(i): [feature] for i, feature in enumerate(features)}\n","    dataframe = pd.DataFrame.from_dict(features_df)\n","    return dataframe\n","\n","#Extract the operation codes from a file\n","def extract_op_codes(filepath):\n","  \"\"\"\n","  This function extracts operation codes from a binary file deleting info about operands\n","\n","  @param filepath the file path where the binary file is located\n","  @return opcodelist the list of operation extracted from the binary\n","  \"\"\"\n","  r = radare.open(filepath)\n","  # Analyze all\n","  r.cmd(\"aaaa\")\n","  # extract the opcodes without the operands (~[0] selects the first word only)\n","  a = r.cmd(\"pif @@f ~[0]\")\n","  # splitting lines\n","  a = a.split(\"\\n\")\n","  opcodelist = \" \".join(a)\n","  r.quit()\n","  return opcodelist\n","\n","\n","def extract_features(input_directory, output_file):\n","  \"\"\"\n","  This function extract the features of all the files contained in a directory and assign a label\n","  to the extracted feature (0 or 1) if the extracted file is in a directory called \"benign\" or not\n","  generating a csv file in output where each line is organized as features[:-1] labels[-1].\n","  The first line will contain the feature names and the label name in the first row.\n","\n","  @param input_directory the input directory where the subdirectory containg binaries must be\n","  @param output_file the output csv file where the output will be stored\n","  \"\"\"\n","  dataframes = pd.DataFrame()\n","  data1 = pd.DataFrame()\n","  data2 = pd.DataFrame()\n","  labels = []\n","  classes = glob.glob(input_directory+'/*')\n","  for i, class_path in enumerate(classes):\n","    class_name = class_path.replace(input_directory,'')\n","    files = glob.glob(class_path+'/*')\n","    #This loop iterates in whole directory and gets all the feature from the exe files\n","    for sample in tqdm(glob.glob(class_path+'/*'), desc=f'({i+1}\\{len(classes)}) {class_name}'):\n","      data1 = pd.concat([pd.DataFrame(dataframes), get_ember_features(sample)])\n","      data2 = pd.concat([pd.DataFrame(data2), data1])\n","      if(\"benign\" in class_path):\n","        labels.append(0)  #Labeling bening as 0\n","      else:\n","        labels.append(1) #Labeling evil as 1\"\"\"\n","  #This instruction assign the label to the feature extracted\n","  data3 = data2.assign(Labels = labels)\n","\n","  #This instruction transform a pandas dataframe to a csv\n","  data3.to_csv(output_file)\n","\n","def shuffle_my_dataset(datasetcsv:str, new_datasetcsv:str):\n","    \"\"\"\n","    This function shuffles all the line of a file except the first line\n","\n","    @param datasetcsv the file to shuffle\n","    @param new_datasetcsv the shuffled file\n","    @return: the number of shuffoled lines\n","    \"\"\"\n","    lines = open(datasetcsv).readlines()\n","    first_line=lines[0]\n","    feature_lines = lines[1:]\n","    random.shuffle(feature_lines)\n","    open(new_datasetcsv, 'w').writelines(first_line)\n","    open(new_datasetcsv, 'a').writelines(feature_lines)\n","    return(len(feature_lines))\n","\n","def load_my_dataset(datasetcsv:str):\n","    \"\"\"\n","    This function given a dataset of features in form of a csv.\n","    Each line must be organized as features[:-1] labels[-1].\n","\n","    @param: datasetcsv the file csv where is stored the dataset\n","    @return: dataset as Bunch\n","    \"\"\"\n","    with open(datasetcsv) as csv_file:\n","        data_reader = csv.reader(csv_file)\n","        feature_names = next(data_reader)[:-1]\n","        data = []\n","        target = []\n","        for row in data_reader:\n","            # Check if the row is't corrupted\n","            if(row.count('') <=0):\n","                features = row[:-1]\n","                label = row[-1]\n","                data.append([float(num) for num in features])\n","                target.append(int(label))\n","            \n","        data = np.array(data)\n","        target = np.array(target)\n","    return Bunch(data=data, target=target, feature_names=feature_names)\n","\n","def count_feature_importance(featurecsv:str):\n","    \"\"\"\n","    This function count the number of feature that have as importance 0\n","\n","    @param featurecsv the input feature dict to read and cout\n","    @return: the number of feature that have 0 importance\n","    \"\"\"\n","    zero_importance=0\n","    with open(featurecsv) as csv_file:\n","        data_reader = csv.reader(csv_file)\n","        for row in data_reader:\n","            if(row.count('') <=0):\n","                if(float(row[1])==float(0)):\n","                    zero_importance_features+=1\n","\n","    return zero_importance_features\n","\n","def count_my_dataset(datasetcsv:str):\n","    \"\"\"\n","    This function count the elements of the dataset and divides them in evil and good \n","    based on their label and prints the numbers and the percentage of the samples of the 2 classes\n","\n","    @param datasetcsv the input dataset to read and cout\n","    \"\"\"\n","    with open(datasetcsv) as csv_file:\n","        data_reader = csv.reader(csv_file)\n","        feature_names = next(data_reader)[:-1]\n","        data = []\n","        target = []\n","        evil_count=0\n","        good_count=0\n","        for row in data_reader:\n","            if(row.count('') <=0):\n","                if(int(row[-1])==1):\n","                    evil_count+=1\n","                else:\n","                    good_count+=1\n","    print(\"SAMPLES -> evil: \", evil_count, \"good: \",good_count, )\n","    total_count=evil_count+good_count\n","    evil_percentage = (evil_count/total_count)*100\n","    good_percentage= 100-evil_percentage\n","    print(\"PERCENTAGE -> evil: %.2f\" %evil_percentage, \"good: %.2f\" %good_percentage)\n","\n","def delete_usless_features(featurecsv:str, datasetcsv:str, new_datasetcsv:str):\n","    \"\"\"\n","    This function takes an input dataset strips the column line and deletes the \n","    colum that have feature importance  equal to 0 in the featurecsv and saves the new\n","    dataset in new_datasetcsv\n","\n","    @param featurecsv the input feature dict to read\n","    @param datasetcsv the input dataset to read\n","    @param new_datasetcsv the input new dataset to write\n","    \"\"\"\n","\n","    features_todel=[]\n","    with open(featurecsv) as csv_file:\n","        data_reader = csv.reader(csv_file)\n","        for row in data_reader:\n","            if(row.count('') <=0):\n","                if(float(row[1])==float(0)):\n","                    features_todel.append(row[0])\n","    data = pd.read_csv(datasetcsv)\n","    for feature in features_todel:\n","        data.drop(feature, inplace=True, axis=1)\n","    data = data.iloc[: , 1:]\n","    data.to_csv(new_datasetcsv)\n","    \n","def eval_accuracy(dataset,feature_importances,model_path):\n","    \"\"\"\n","    This function evaluate the accuracy of a random forest on the specified dataset\n","\n","    @param dataset the dataset to evaluate (in the directory is necessary to divide malware and benign\n","                    according to the load_my_dataset function)\n","    @param feature_importances the list of the ember feature and the importance used during the training \n","                                of the model\n","    @param model_path the model that will evalutate the malwares\n","\n","    @return acc the value of the accuracy on the given dataset of the model\n","    \"\"\"\n","\n","    extract_features(dataset, \"../accuracy_eval.csv\")\n","    count_my_dataset(\"../accuracy_eval.csv\")\n","    shuffle_my_dataset(\"../accuracy_eval.csv\",\"../accuracy_eval.csv\")\n","    delete_usless_features(feature_importances, \"../accuracy_eval.csv\", \"../accuracy_eval.csv\")\n","    \n","    gamma_set = load_my_dataset(\"../accuracy_eval.csv\")\n","    X_gamma = gamma_set.data\n","    y_gamma = gamma_set.target\n","    forest = load(model_path)\n","    y_pred = forest.predict(X_gamma)\n","    acc = accuracy_score(y_gamma, y_pred)\n","\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"umA_18Dc3Gr_"},"source":["## Feature extraction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65awuUee3GsQ"},"outputs":[],"source":["#Change it with the path of your directory with samples to extract,remember to put it in a \"malware\" folder and \"benign\" floder\n","#extract_features(\"/home/ubuntu/AI4C/test\", \"/home/ubuntu/Desktop/Newg/features_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kq_MJdwq3GsU"},"outputs":[],"source":["#Change it with the path of your directory with the csv to shuffle\n","#shuffle_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_validation.csv\",\"/home/dario/AI4C/RF/Datasets/features_s_validation.csv\")\n","#shuffle_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_test.csv\",\"/home/dario/AI4C/RF/Datasets/features_s_test.csv\")\n","#shuffle_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_train.csv\",\"/home/dario/AI4C/RF/Datasets/features_s_train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0-lMwK_3Gsb"},"outputs":[],"source":["#Sanity check, change the csv names\n","print(\"TEST_SET\")\n","count_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_test.csv\")\n","print(\"TRAIN_SET\")\n","count_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_train.csv\")\n","print(\"VALIDATION_SET\")\n","count_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_validation.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osnZsrRi3Gsg"},"outputs":[],"source":["#Proof of concept of feature importances\n","\n","forest = load(\"/home/dario/AI4C/RF/Models/randomforest1.joblib\")\n","train_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_validation.csv\")\n","feature_names = train_set.feature_names\n","importances = forest.feature_importances_\n","\n","forest_importances = pd.Series(importances, index=feature_names)\n","std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n","\n","fig, ax = plt.subplots()\n","forest_importances.plot.bar(yerr=std, ax=ax)\n","ax.set_title(\"Feature importances\")\n","ax.set_ylabel(\"Mean decrease in impurity\")\n","plt.tick_params(\n","    axis='x',          \n","    which='both',      \n","    bottom=False,      \n","    top=False,         \n","    labelbottom=False) \n","fig.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGC4KYJi3Gsw"},"outputs":[],"source":["#Finding the most important features using a greedy aproach\n","\n","SAMPLE_SPLIT = 2 \n","MAX_FEATURES = \"sqrt\" \n","MAX_E=200+5\n","MAX_DEPTH = 3\n","\n","train_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_d_train.csv\")\n","test_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_d_test.csv\")\n","validation_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_d_validation.csv\")\n","\n","X_train = train_set.data\n","X_test = test_set.data\n","X_validation = validation_set.data\n","\n","y_train = train_set.target\n","y_test = test_set.target\n","y_validation = validation_set.target\n","\n","feature_names = validation_set.feature_names\n","feature_dict ={feature_names[i]:0 for i in range(len(feature_names))}\n","accuracy_top = 0\n","accuracy=[]\n","for num in range(5, MAX_E, 5):\n","    print(\"step: \", num/5,\"/\", MAX_E/5-1)\n","    for x in range(20): # Just for safety\n","        forest = RandomForestClassifier(n_estimators=num, max_features=MAX_FEATURES, max_depth=MAX_DEPTH, min_samples_split=SAMPLE_SPLIT)\n","        # Training\n","        forest = forest.fit(X_train, y_train)\n","        \n","        # Testing\n","        y_pred = forest.predict(X_test)\n","        accuracy.append(accuracy_score(y_test, y_pred))\n","        test_acc = accuracy_score(y_test, y_pred)\n","        \n","\n","        # Validate\n","        y_pred = forest.predict(X_validation)\n","        acc = accuracy_score(y_validation, y_pred)\n","        if(acc >accuracy_top): #Saving the best model so far\n","            print(\"Best performances: \",acc,\" -> \", num, \"for test accuracy -> \", test_acc)\n","            dump(forest,\"/home/dario/AI4C/RF/Models/RandomForestBest4.joblib\")\n","            accuracy_top = acc\n","\n","        # Find importances\n","        importances = forest.feature_importances_\n","        forest_importances = pd.Series(importances, index=feature_names)\n","        std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n","        std = list(std)\n","\n","        #Update importances\n","        iterator=0\n","        for key in feature_dict.keys():\n","            if(std[iterator]>feature_dict[key]):\n","                feature_dict[key]=std[iterator]\n","            iterator=iterator+1\n","with open('FeatureImportances4.csv', 'w') as f:\n","    for key in feature_dict.keys():\n","        f.write(\"%s, %s\\n\" % (key, feature_dict[key]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_coqruhV3Gsy"},"outputs":[],"source":["#Sanity check,Count the number of 0 importance features chenge path to your csv\n","\n","print(count_feature_importance(\"FeatureImportances2.csv\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOeK7aid3Gs8"},"outputs":[],"source":["#Delete usless features, change the path to your csv\n","\n","#delete_usless_features(\"FeatureImportances2.csv\", \"/home/dario/AI4C/RF/Datasets/features_s_train.csv\", \"/home/dario/AI4C/RF/Datasets/features_d_train.csv\")"]},{"cell_type":"markdown","metadata":{"id":"hwplUypr3GtS"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WdTicUR3GtZ"},"outputs":[],"source":["#Training procedure prepare at best your csv before doing this\n","\n","SAMPLE_SPLIT = 2 \n","MAX_FEATURES = \"log2\" # this and not \"sqrt\" for boost performances if \"sqrt\" is used in feature evaluation\n","NUM_ESTIMATORS = 100\n","MAX_DEPTH = 3\n","\n","# Data loading\n","train_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_d_train.csv\")\n","test_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_d_test.csv\")\n","\n","X_train = train_set.data\n","X_test = test_set.data\n","y_train = train_set.target\n","y_test = test_set.target\n","feature_names = train_set.feature_names\n","\n","# Creating a random forest\n","rf = RandomForestClassifier(n_estimators=NUM_ESTIMATORS, max_depth=MAX_DEPTH, max_features=MAX_FEATURES, min_samples_split=SAMPLE_SPLIT)\n","# Training the forest\n","classifier = rf.fit(X_train, y_train)\n","\n","# Testing\n","y_pred = classifier.predict(X_test)\n","\n","#print out all the results\n","print(\"%d exact prediction over %d samples\"\n","     % ((y_test == y_pred).sum(), X_test.shape[0]))\n","print(\"Normalized confusion matrix \\n %s\" % (confusion_matrix(y_test, y_pred, normalize='true')))\n","print(\"Classification report\\n %s\" % (classification_report(y_test, y_pred)))\n","print(\"Accuracy score: %.3f\" % (accuracy_score(y_test, y_pred)))\n","dump(classifier,\"/home/dario/AI4C/RF/Models/RFtest.joblib\")"]},{"cell_type":"markdown","metadata":{"id":"hk36Szoz3Gtq"},"source":["## Generalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LML7y5fM3Gtr","outputId":"48031978-201e-43a4-e275-95b6c35c07ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generalization performance -> 0.797\n"]}],"source":["#Test the generalization of the net on a dataset of validation\n","\n","generalization_set = load_my_dataset(\"/home/dario/AI4C/RF/Datasets/features_d_validation.csv\")\n","X_generalization = generalization_set.data\n","y_generalization = generalization_set.target\n","\n","forest = load(\"/home/dario/AI4C/RF/Models/RF.joblib\")\n","\n","y_pred = forest.predict(X_generalization)\n","acc = accuracy_score(y_generalization, y_pred)\n","print(\"Generalization performance ->\", acc)\n","\n","#could use eval_accuracy"]},{"cell_type":"markdown","metadata":{"id":"teFEVuEN3Gtw"},"source":["## GAMMA Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7HmmkEw3Gtx"},"outputs":[],"source":["accuracy=eval_accuracy(\"/home/dario/AI4C/RF/mario/malwareb\",\"/home/dario/AI4C/RF/CODES/FeatureImportances2.csv\", \"/home/dario/AI4C/RF/Models/RF.joblib\")\n","print(\"accuracy after gamma attack -> \"+ str(accuracy))"]}],"metadata":{"kernelspec":{"display_name":"AI4C","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"51adf068da8bd79e1b4dd3cfd51c667ab4bf056ccd84bf8dbe6ef0731d1fbab7"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}